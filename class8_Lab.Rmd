---
title: "Class8 Lab"
author: "Vicky Yue"
date: "9/24/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Description

Data on driving incidents was collected for all 52 states in the United states.
This lab will explore whether or not state-level factors contribute to Car insurance premiums.
The original goal of this dataset was to determine which state had the worst drivers.

<p align="center">
  <img width="320" height="200" src="https://github.com/mhc-stat340-f2019-sec02/class8_LAB/blob/master/featured.jpeg">
</p>

## Organization
Instead of a private repo per student, we're going to work collaboratively in a single repository.
You can work by yourself or with another student in a team.

On the GitHub repository page for this lab, click "Fork" in the top right corner. This will create a copy of the lab repository in your own account. You will then clone this repository to RStudio. If you're working in a team, only one of you needs to fork the repository. Once you have cloned the repository, create a new .Rmd file with a name like "Lab02_teamname.Rmd", specific to your team. In that R Markdown file, complete the Lab Tasks and Discussion items outlined below. Then commit and push your work to GitHub. Your work will go to your forked version of the repository. Once you're ready, submit a pull request to merge your work back into the main class repository.

## Due date

You must submit your pull request by 2019-09-27 at 23:59:59.

## LAB tasks

* **Read in the data set `data/bad-drivers.csv`**
  * Name your dataset, for example, `badDrivers <- read.csv("./data/bad-drivers.csv")`
  * (recommended) rename the columns to shorter nicknames
  

```{r}
badDrivers <- read.csv("./data/bad-drivers.csv")
# Rename a columns in R
names(badDrivers) <- c("State", "badDriverNum","speedingPer","alcoholPer","undistractedPer","noAccPer","insurance","lossIncur")
show(badDrivers)
```
* **Exploratory data analysis**
  * Create a draftsman plot showing all pairwise comparisons between columns in the data.
  * Present a brief description of trends you see in the data, and how they may influence fitting a model.
  * Plot the estimated distribution of _Percentage of Drivers Involved in Fatal Collisions who were speeding_ using a histogram.
	  * If you include the above covariate as an explanatory variable in your regression (part of the X), will the distribution impact your model fit?
```{r}
plot(badDrivers)
library(ggplot2)
ggplot(data = badDrivers,
mapping = aes(x =  speedingPer),
) +
geom_histogram()
lm(badDriverNum~speedingPer, data = badDrivers)
```
From the draftsman plot, we can tell that insurance and lossInur are related positively. Also, undistracted percentage might not have a strong relationship with any of the variable but will have a great impact on the fiited model. 


* **Regression analysis**
  * The target variable for our regression models is `Car Insurance Premiums (CIP)`, measured in dollars.
  * Pick a covariate you feel is most related to Car Insurance Premiums (I'll call this `M` for most related)  noAccPer
  * Fit a simple linear regression `(lm)` model that related **CIP** to **M** and save this model as `reg01`. 
  * Fit a multiple linear regression model `(lm)` that includes **M** and save this as `reg02`.
  * Fit a polynomial regression model `(lm)` relating **CIP** to **M** and save this as `reg03`.
  * Pick a model from REG01, REG02, and REG03. Plot **M** by **CIP**, and overlay the chosen model's fitted values.
  * Describe your model. Do all the variables significantly contribute to predicting **CIP**? Interpret the coefficients, their direction (positive or negative) and how they relate to **CIP**.
  * How does your multiple regression model compare the your simple linear regression, and how would communicate these results to an audience?
```{r}
reg01 = lm(insurance~noAccPer, data = badDrivers)
reg02 = lm(insurance~noAccPer+lossIncur, data = badDrivers)
reg03 = lm(insurance~noAccPer+(noAccPer^2), data = badDrivers)
library(ggplot2)
ggplot(data = badDrivers,
mapping = aes(x = noAccPer+lossIncur, y = insurance),
) +
geom_point()+geom_smooth(method = lm)
summary(reg02)
summary(reg01)
```
No. The "Percentage Of Drivers Involved In Fatal Collisions Who Had Not Been Involved In Any Previous Accidents" doesn't have significant contribution to CIP.The noAccPer and lossIncur both have positive relationship with CIP. However, the p-value for noAccPer is too large(>0.05) to provide evidence that noAccPer is related to CIP.

While the simple linear regression that has noAccPer as the independent variable didn't do a good job explaining CIP, the fitted multiple linear regression model is better and more liable(mainly because of the second variable "lossIncur"). Statistically, the R-squared, which is basically the percentage of how well the (variance of) independent variable explains the dependent variable, of the MLR is higher than the SLR we fitted. P-value is also used to weigh the strength of the evidence that data tell us about the population. The small p-value(<0.05) in MLR indicates strong evidence that the model is sufficient to make reliable prediction for CIP(reject the null hypothesis).


* **Hold-out**
  * Randomly select, and remove, 10 states from the training set. Store these 10 states in a dataset called `holdOut` and remaining 41 states in a dataset called `training`.
     * `badDrivers[c(1,2,3),]` selects the first, second, and third observation from your dataset.
     *  Take a look at the `sample` command in **R**
  * Re-train REG01,REG02,REG03 on `training`
  * For REG01, 02, and 03, compute the mean-squared error (MSE) on `holdOut`
  * Which model would you select and why?
```{r}
library(caret)
library(mlbench)
smp_siz = floor(41/51*nrow(badDrivers))
set.seed(123)
train_ind = sample(seq_len(nrow(badDrivers)),size = smp_siz)

training <- badDrivers[ train_ind,]
holdOut <- badDrivers[-train_ind,]

reg01 = lm(insurance~noAccPer, data = training)
reg02 = lm(insurance~noAccPer+lossIncur, data = training)
reg03 = lm(insurance~noAccPer+(noAccPer^2), data = training)

sm1 <- summary(reg01)
sm2 <- summary(reg02)
sm3 <- summary(reg03)

mse <- function(sm) 
mean(sm$residuals^2)

mse(sm1)
mse(sm2)
mse(sm3)
```
I would select reg02, whose MSE is the smallest and indicates the trained model did a better job in fitting the testing data. A reason why that the MSE in reg01 and reg03 are ridiculously high is because in the former analysis we conclude that there is hardly any relationship between insurance and noAccPer, so it is predictable that the model cannot be applicable in the testing data as well.

* **Cross-validation**
  * For REG01, REG02, REG03
    * Split your data into 5 training/testing sets (note, one dataset will have 11 observations)
	* Create an empty data.frame called `crossValResults` that has 3 columns (one for each model) and 5 rows (one for each test MSE)
    * Program a for loop that
	      * *trains* your model on 4 pieces of the data
		  * *tests*, or makes predictions, on the "held-out" dataset. 
		  * *computes* the MSE on the "held-out" dataset
		  * *stores* the test MSE in `crossValResults`. 
    * When completed, you should have computed 15 MSEs, 5 for every regression model stored as columns in a data frame.
    * Compute the CV error for your regression models, the MSE averaged over each test set.
	* How does the CV error compare to the hold-out error?
	* How does the Cross-validation MSE compare between your simple and multiple regression?
```{r}
smp_siz = floor(10/51*nrow(badDrivers))
set.seed(123)
train_ind = sample(seq_len(nrow(badDrivers)),size = smp_siz)
#the 1st set
set1 <- badDrivers[ train_ind,]
#whatever data is left
left <- badDrivers[-train_ind,]

smp_siz = floor(10/41*nrow(left))
set.seed(123)
train_ind = sample(seq_len(nrow(left)),size = smp_siz)
set2 <- left[ train_ind,]
left <- left[-train_ind,]

smp_siz = floor(10/31*nrow(left))
set.seed(123)
train_ind = sample(seq_len(nrow(left)),size = smp_siz)
set3 <- left[ train_ind,]
left <- left[-train_ind,]

smp_siz = floor(10/21*nrow(left))
set.seed(123)
train_ind = sample(seq_len(nrow(left)),size = smp_siz)
set4 <- left[ train_ind,]
set5 <- left[-train_ind,]

#create empty data frames
crossValResults   <- NULL
for(i in 1:5)
{
   row<-c(a=0,b=0,c=0)
   crossValResults = rbind(crossValResults, row)
}
show(crossValResults)

mse <- function(sm) 
mean(sm$residuals^2)

#trains SLR,MLR and Pr
reg001 = lm(insurance~noAccPer, data = rbind(set1,set2,set3,set4))
test001 = lm(insurance~noAccPer, data = set5)
reg010 = lm(insurance~noAccPer+lossIncur, data = rbind(set1,set2,set3,set4))
test010 = lm(insurance~noAccPer+lossIncur, data = set5)
reg100 = lm(insurance~noAccPer+(noAccPer^2), data = rbind(set1,set2,set3,set4))
test100 = lm(insurance~noAccPer+(noAccPer^2), data = set5)
sm001 <- summary(test001)
sm010 <- summary(test010)
sm100 <- summary(test100)
mse(sm001)
mse(sm010)
mse(sm100)


reg002 = lm(insurance~noAccPer, data = rbind(set1,set2,set3,set5))
test002 = lm(insurance~noAccPer, data = set4)
reg020 = lm(insurance~noAccPer+lossIncur, data = rbind(set1,set2,set3,set5))
test020 = lm(insurance~noAccPer+lossIncur, data = set4)
reg200 = lm(insurance~noAccPer+(noAccPer^2), data = rbind(set1,set2,set3,set5))
test200 = lm(insurance~noAccPer+(noAccPer^2), data = set4)
sm002 <- summary(test002)
sm020 <- summary(test020)
sm200 <- summary(test200)
mse(sm002)
mse(sm020)
mse(sm200)

reg003 = lm(insurance~noAccPer, data = rbind(set1,set2,set5,set4))
test003 = lm(insurance~noAccPer, data = set3)
reg030 = lm(insurance~noAccPer+lossIncur, data = rbind(set1,set2,set5,set4))
test030 = lm(insurance~noAccPer+lossIncur, data = set3)
reg300 = lm(insurance~noAccPer+(noAccPer^2), data = rbind(set1,set2,set5,set4))
test300 = lm(insurance~noAccPer+(noAccPer^2), data = set3)
sm003 <- summary(test003)
sm030 <- summary(test030)
sm300 <- summary(test300)
mse(sm003)
mse(sm030)
mse(sm300)

reg004 = lm(insurance~noAccPer, data = rbind(set1,set5,set3,set4))
test004 = lm(insurance~noAccPer, data = set2)
reg040 = lm(insurance~noAccPer+lossIncur, data = rbind(set1,set5,set3,set4))
test040 = lm(insurance~noAccPer+lossIncur, data = set2)
reg400 = lm(insurance~noAccPer+(noAccPer^2), data = rbind(set1,set5,set3,set4))
test400 = lm(insurance~noAccPer+(noAccPer^2), data = set2)
sm004 <- summary(test004)
sm040 <- summary(test040)
sm400 <- summary(test400)
mse(sm004)
mse(sm040)
mse(sm400)

reg005 = lm(insurance~noAccPer, data = rbind(set5,set2,set3,set4))
test005 = lm(insurance~noAccPer, data = set1)
reg050 = lm(insurance~noAccPer+lossIncur, data = rbind(set5,set2,set3,set4))
test050 = lm(insurance~noAccPer, data = set1)
reg500 = lm(insurance~noAccPer+(noAccPer^2), data = rbind(set5,set2,set3,set4))
test500 = lm(insurance~noAccPer+(noAccPer^2), data = set1)
sm005 <- summary(test005)
sm050 <- summary(test050)
sm500 <- summary(test500)
mse(sm005)
mse(sm050)
mse(sm500)

mse_SLR = mse(sm001) + mse(sm002) + mse(sm003) + mse(sm004) + mse(sm005)/5
mse_MLR = mse(sm010) + mse(sm020) + mse(sm030) + mse(sm040) + mse(sm050)/5
mse_PR = mse(sm100) + mse(sm200) + mse(sm300) + mse(sm400) + mse(sm500)/5


```
In general, I think the CV error is smaller than the hold-out error. 
I think basically cross-validation is doing linear regression and multiple regression repetitively on the same but divided dataset, which allows it to analyze multiple times in order to map a more accurate pattern of the data. 

<!-- ## Brief Report -->

<!-- * Paragraph 1: Describe the dataset --> The dataset bad-Drivers contains 8 variables that are observed in 51 states. They are State,Number of drivers involved in fatal collisions per billion miles,Percentage Of Drivers Involved In Fatal Collisions Who Were Speeding,Percentage Of Drivers Involved In Fatal Collisions Who Were Alcohol-Impaired,Percentage Of Drivers Involved In Fatal Collisions Who Were Not Distracted,Percentage Of Drivers Involved In Fatal Collisions Who Had Not Been Involved In Any Previous Accidents,Car Insurance Premiums ($),and Losses incurred by insurance companies for collisions per insured driver ($).

<!-- * Paragraph 2: Describe the models you'll use to make predictions. --> I'll use Linear regression, multiple linear regression, and polynomial regression. 
<!-- * Paragraph 3: Summarize the methods you'll use to decide which model is best. Make sure to define and describe the test metric (MSE) --> The first method I'm going to use is Hold-out, which is divide the dataset into testing and training data. Testing data can be used to check the accuracy of the model fitted from the training data. The other method is K-fold cross validation. We divide the dataset into smaller separate n subsets. While one of the set is set as the testing data, the rest n-1 subsets would be used to fit a model. Through our training and testing, we wish to find the model that have the least MSE, the average squared difference between the estimated value and actual value. 
<!-- * Paragraph 4:  -->


References

https://stackoverflow.com/questions/36068963/r-how-to-split-a-data-frame-into-training-validation-and-test-sets
https://stackoverflow.com/questions/48567630/create-an-empty-data-frame-without-specifying-column-names